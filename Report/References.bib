@article{Ashraf2009,
abstract = {Pain is typically assessed by patient self-report. Self-reported pain, however, is difficult to interpret and may be impaired or in some circumstances (i.e., young children and the severely ill) not even possible. To circumvent these problems behavioral scientists have identified reliable and valid facial indicators of pain. Hitherto, these methods have required manual measurement by highly skilled human observers. In this paper we explore an approach for automatically recognizing acute pain without the need for human observers. Specifically, our study was restricted to automatically detecting pain in adult patients with rotator cuff injuries. The system employed video input of the patients as they moved their affected and unaffected shoulder. Two types of ground truth were considered. Sequence-level ground truth consisted of Likert-type ratings by skilled observers. Frame-level ground truth was calculated from presence/absence and intensity of facial actions previously associated with pain. Active appearance models (AAM) were used to decouple shape and appearance in the digitized face images. Support vector machines (SVM) were compared for several representations from the AAM and of ground truth of varying granularity. We explored two questions pertinent to the construction, design and development of automatic pain detection systems. First, at what level (i.e., sequence- or frame-level) should datasets be labeled in order to obtain satisfactory automatic pain detection performance? Second, how important is it, at both levels of labeling, that we non-rigidly register the face? © 2009 Elsevier B.V. All rights reserved.},
author = {Ashraf, Ahmed Bilal and Lucey, Simon and Cohn, Jeffrey F. and Chen, Tsuhan and Ambadar, Zara and Prkachin, Kenneth M. and Solomon, Patricia E.},
doi = {10.1016/j.imavis.2009.05.007},
file = {:C$\backslash$:/Users/Kasper/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashraf et al. - 2009 - The painful face - Pain expression recognition using active appearance models.pdf:pdf},
isbn = {9781595938176},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Active appearance models,Automatic facial image analysis,FACS,Facial expression,Pain,Support vector machines},
number = {12},
pages = {1788--1796},
pmid = {22837587},
publisher = {Elsevier B.V.},
title = {{The painful face - Pain expression recognition using active appearance models}},
url = {http://dx.doi.org/10.1016/j.imavis.2009.05.007},
volume = {27},
year = {2009}
}
@article{Kaltwang2012,
abstract = {Automatic pain recognition is an evolving research area with promising applications in health care. In this paper, we propose the first fully automatic approach to continuous pain intensity estimation from facial images. We first learn a set of independent regression functions for continuous pain intensity estimation using different shape (facial landmarks) and appearance (DCT and LBP) features, and then perform their late fusion. We show on the recently published UNBC-MacMaster Shoulder Pain Expression Archive Database that late fusion of the afore-mentioned features leads to better pain intensity estimation compared to feature-specific pain intensity estimation.},
author = {Kaltwang, Sebastian and Rudovic, Ognjen and Pantic, Maja},
doi = {10.1007/978-3-642-33191-6\_36},
file = {:C$\backslash$:/Users/Kasper/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaltwang, Rudovic, Pantic - 2012 - Continuous pain intensity estimation from facial expressions.pdf:pdf},
isbn = {9783642331909},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {368--377},
title = {{Continuous pain intensity estimation from facial expressions}},
volume = {7432 LNCS},
year = {2012}
}
@article{Lenc2014,
author = {Lenc, A. Vedaldi and K.},
journal = {CoRR},
title = {{MatConvNet - Convolutional Neural Networks for MATLAB}},
volume = {1412.4564},
year = {2014}
}
@inproceedings{Lucey2012,
abstract = {In intensive care units in hospitals, it has been recently shown that enormous improvements in patient outcomes can be gained from the medical staff periodically monitoring patient pain levels. However, due to the burden/stress that the staff are already under, this type of monitoring has been difficult to sustain so an automatic solution could be an ideal remedy. Using an automatic facial expression system to do this represents an achievable pursuit as pain can be described via a number of facial action units (AUs). To facilitate this work, the "University of Northern British Columbia-McMaster Shoulder Pain Expression Archive Database" was collected which contains video of participant's faces (who were suffering from shoulder pain) while they were performing a series of range-of-motion tests. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well. To promote and facilitate research into pain and augmentcurrent datasets, we have publicly made available a portion of this database, which includes 200 sequences across 25 subjects, containing more than 48,000 coded frames of spontaneous facial expressions with 66-point AAM tracked facial feature landmarks. In addition to describing the data distribution, we give baseline pain and AU detection results on a frame-by-frame basis at the binary-level (i.e. AU vs. no-AU and pain vs. no-pain) using our AAM/SVM system. Another contribution we make is classifying pain intensities at the sequence-level by using facial expressions and 3D head pose changes. © 2011 Elsevier B.V. All rights reserved.},
author = {Lucey, Patrick and Cohn, Jeffrey F. and Prkachin, Kenneth M. and Solomon, Patricia E. and Chew, Sien and Matthews, Iain},
booktitle = {Image and Vision Computing},
doi = {10.1016/j.imavis.2011.12.003},
isbn = {0262-8856},
issn = {02628856},
keywords = {Action Units (AUs),Active Appearance Models (AAMs),FACS,Pain},
number = {3},
pages = {197--205},
title = {{Painful monitoring: Automatic pain monitoring using the UNBC-McMaster shoulder pain expression archive database}},
volume = {30},
year = {2012}
}
@article{Prkachin1992,
abstract = {A number of facial actions have been found to be associated with pain. However, the consistency with which these actions occur during pain of different types has not been examined. This paper focuses on the consistency of facial expressions during pain induced by several modalities of nociceptive stimulation. Forty-one subjects were exposed to pain induced by electric shock, cold, pressure and ischemia. Facial actions during painful and pain-free periods were measured with the Facial Action Coding System. Four actions showed evidence of a consistent association with pain, increasing in likelihood, intensity or duration across all modalities; brow lowering, tightening and closing of the eye lids and nose wrinkling/upper lip raising. Factor analyses suggested that the facial actions reflected a general factor with a reasonably consistent pattern across modalities which could be combined into a sensitive single measure of pain expression. The findings suggest that the 4 actions identified carry the bulk of facial information about pain. They also provide evidence for the existence of a universal facial expression of pain. Implications of the findings for the measurement of pain expression are discussed.},
author = {Prkachin, K. M.},
doi = {10.1016/0304-3959(92)90213-U},
file = {:C$\backslash$:/Users/Kasper/Documents/GitHub/TIENPRAU/Litterature/The consistency of facial expressions of pain - a comparison across modalities.pdf:pdf},
isbn = {0304-3959 (Print)$\backslash$n0304-3959 (Linking)},
issn = {03043959},
journal = {Pain},
keywords = {Facial Action Coding System,Facial expression,Nociceptive stimulation,Pain},
pages = {297--306},
pmid = {1491857},
title = {{The consistency of facial expressions of pain: A comparison across modalities}},
volume = {51},
year = {1992}
}
@article{Prkachin2008a,
abstract = {The present study examined psychometric properties of facial expressions of pain. A diverse sample of 129 people suffering from shoulder pain underwent a battery of active and passive range-of-motion tests to their affected and unaffected limbs. The same tests were repeated on a second occasion. Participants rated the maximum pain induced by each test on three self-report scales. Facial actions were measured with the Facial Action Coding System. Several facial actions discriminated painful from non-painful movements; however, brow-lowering, orbit tightening, levator contraction and eye closing appeared to constitute a distinct, unitary action. An index of pain expression based on these actions demonstrated test-retest reliability and concurrent validity with self-reports of pain. The findings support the concept of a core pain expression with desirable psychometric properties. They are also consistent with the suggestion of individual differences in pain expressiveness. Reasons for varying reports of relations between pain expression and self-reports in previous studies are discussed. ?? 2008 International Association for the Study of Pain.},
author = {Prkachin, Kenneth M. and Solomon, Patricia E.},
doi = {10.1016/j.pain.2008.04.010},
file = {:C$\backslash$:/Users/Kasper/Documents/GitHub/TIENPRAU/Litterature/The structure, reliability and validity of pain expression Evidence from patients with shoulder pain.pdf:pdf},
isbn = {1872-6623 (Electronic) 0304-3959 (Linking)},
issn = {03043959},
journal = {Pain},
keywords = {Facial Action Coding System,Facial expression,Pain,Pain expression,Shoulder pain},
number = {2},
pages = {267--274},
pmid = {18502049},
publisher = {International Association for the Study of Pain},
title = {{The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain}},
url = {http://dx.doi.org/10.1016/j.pain.2008.04.010},
volume = {139},
year = {2008}
}
@inproceedings{Rudovic2011,
abstract = {Given the facial points extracted from an image of a face in an arbitrary pose, the goal of facial-point-based head-pose normalization is to obtain the corresponding facial points in a predefined pose (e.g., frontal). This involves inference of complex and high-dimensional mappings due to the large number of the facial points employed, and due to differences in head-pose and facial expression. Most regression-based approaches for learning such mappings focus on modeling correlations only between the inputs (i.e., the facial points in a non-frontal pose) and the outputs (i.e., the facial points in the frontal pose), but not within the inputs and the outputs of the model. This makes these models prone to errors due to noise and outliers in test data, often resulting in anatomically impossible facial configurations formed by their predictions. To address this, we propose Shape-constrained Gaussian Process (SC-GP) regression for facial-point-based head-pose normalization. Specifically, a deformable face-shape model is used to learn a face-shape prior, which is placed on both the input and the output of GP regression in order to constrain the model predictions to anatomically feasible facial configurations. Our extensive experiments on both synthetic and real image data show that the proposed approach generalizes well across poses and handles successfully noise and outliers in test data. In addition, the proposed model outperforms previously proposed approaches to facial-point-based head-pose normalization.},
author = {Rudovic, Ognjen and Pantic, Maja},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126407},
isbn = {9781457711015},
issn = {1550-5499},
pages = {1495--1502},
title = {{Shape-constrained Gaussian process regression for facial-point-based head-pose normalization}},
year = {2011}
}
@inproceedings{Shen2012a,
abstract = {Visual attention is the ability to select visual stimuli that are most behaviorally relevant among the many others. It allows us to allocate our limited processing resources to the most informative part of the visual scene. In this paper, we learn general high-level concepts with the aid of selective attention in a principled un- supervised framework, where a three layer deep network is built and greedy layer- wise training is applied to learn mid- and high- level features from salient regions of images. The network is demonstrated to be able to successfully learn mean- ingful high-level concepts such as faces and texts in the third-layer and mid-level features like junctions, textures, and parallelism in the second-layer. Unlike pre- trained object detectors that are recently included in saliency models to predict semantic objects, the higher-level features we learned are general base features that are not restricted to one or few object categories. A saliency model built upon the learned features demonstrates its competitive predictive power in natural scenes compared with existing methods.},
author = {Shen, Chengyao and Song, Mingli and Qi, Zhao},
booktitle = {NIPS 2012 Neural Information Processing Systems},
number = {65},
title = {{Learning High-Level Concepts by Training A Deep Network on Eye Fixations}},
year = {2012}
}
